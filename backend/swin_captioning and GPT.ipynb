{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a57487",
   "metadata": {
    "id": "17a57487"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer,\n",
    "    AutoImageProcessor, AutoModelForImageClassification\n",
    ")\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f2ea6",
   "metadata": {
    "id": "fb2f2ea6"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 1: Imports and device\n",
    "# ==============================\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from torchvision import transforms\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3d552",
   "metadata": {
    "id": "39e3d552",
    "outputId": "c2ec2f2c-854d-4e9b-d78e-6577dd8b2770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 88\n",
      "Sample classes: [(0, 'Apple/Alternaria Blotch'), (1, 'Apple/Black Rot'), (2, 'Apple/Brown Spot'), (3, 'Apple/Cedar Apple Rust'), (4, 'Apple/Frog Eye Leaf Spot'), (5, 'Apple/Grey Spot'), (6, 'Apple/Healthy'), (7, 'Apple/Leaf Rust'), (8, 'Apple/Mosaic Virus'), (9, 'Apple/Powdery Mildew')]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Fixed Class Mapping\n",
    "# ==============================\n",
    "DATASET_DIR = r\"D:\\T2430458\\final-dataset\"\n",
    "\n",
    "# Choose one split (e.g., 'train') to build consistent classes\n",
    "SPLIT_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "\n",
    "classes = []\n",
    "\n",
    "for crop_folder in sorted(os.listdir(SPLIT_DIR)):\n",
    "    crop_path = os.path.join(SPLIT_DIR, crop_folder)\n",
    "    if os.path.isdir(crop_path):\n",
    "        for disease_folder in sorted(os.listdir(crop_path)):\n",
    "            disease_path = os.path.join(crop_path, disease_folder)\n",
    "            if os.path.isdir(disease_path):\n",
    "                # Normalize healthy class names\n",
    "                if disease_folder.lower() in [\"fresh leaf\", \"normal leaf\"]:\n",
    "                    disease_folder = \"Healthy\"\n",
    "                classes.append(f\"{crop_folder}/{disease_folder}\")\n",
    "\n",
    "# Build mappings\n",
    "id2label = {i: name for i, name in enumerate(classes)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "crop_model.config.id2label = id2label\n",
    "crop_model.config.label2id = label2id\n",
    "\n",
    "print(\"Number of classes:\", len(classes))\n",
    "print(\"Sample classes:\", list(id2label.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3367bbf",
   "metadata": {
    "id": "a3367bbf",
    "outputId": "b9975e95-d78c-48c4-8f9e-6500dbabc443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captioning models...\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Cell 3: Load Captioning Models\n",
    "# ==============================\n",
    "print(\"Loading captioning models...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-large\"\n",
    ").to(device)\n",
    "\n",
    "vit_model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    \"nlpconnect/vit-gpt2-image-captioning\"\n",
    ").to(device)\n",
    "vit_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "vit_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719483d3",
   "metadata": {
    "id": "719483d3"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 4: Image transforms\n",
    "# ==============================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995d519",
   "metadata": {
    "id": "8995d519"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 5: Caption helpers\n",
    "# ==============================\n",
    "def generate_blip_caption(image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    out = blip_model.generate(**inputs, max_length=50)\n",
    "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_vit_caption(image):\n",
    "    pixel_values = vit_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    out = vit_model.generate(pixel_values, max_length=50)\n",
    "    return vit_tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36adbded",
   "metadata": {
    "id": "36adbded"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 6: Prediction function\n",
    "# ==============================\n",
    "def predict_crop_and_disease(image_path, crop_model, transform):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    crop_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = crop_model(img_tensor).logits\n",
    "        pred_idx = torch.argmax(out, 1).item()\n",
    "        class_name = crop_model.config.id2label[pred_idx]\n",
    "\n",
    "    # Split crop/disease\n",
    "    if \"/\" in class_name:\n",
    "        crop_name, disease_name = class_name.split(\"/\")\n",
    "    else:\n",
    "        crop_name, disease_name = class_name, \"Healthy\"\n",
    "\n",
    "    return crop_name, disease_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59478654",
   "metadata": {
    "id": "59478654"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 7: Build GPT input\n",
    "# ==============================\n",
    "def build_gpt_input(image_path, crop_model, transform, user_question=None):\n",
    "    crop_name, disease_name = predict_crop_and_disease(image_path, crop_model, transform)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    blip_caption = generate_blip_caption(image)\n",
    "    vit_caption = generate_vit_caption(image)\n",
    "    merged_caption = f\"{blip_caption} and {vit_caption}\"\n",
    "\n",
    "    context = f\"The crop is {crop_name} and the disease is {disease_name}. {merged_caption}.\"\n",
    "\n",
    "    if user_question:\n",
    "        return f\"{context} Question: {user_question}\"\n",
    "    else:\n",
    "        return f\"{context} What are the symptoms and cure for this disease? Return JSON with keys 'symptoms' and 'cure'.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3bb75",
   "metadata": {
    "id": "6fd3bb75"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 8: OpenAI GPT Integration\n",
    "# ==============================\n",
    "# API key removed from repository — load from environment variable\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment.\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_gpt(final_input_string, json_mode=True):\n",
    "    if json_mode:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": (\n",
    "                    \"You are an agricultural assistant that answers about crop diseases. \"\n",
    "                    \"Always return answers in JSON format with keys 'symptoms' and 'cure'.\"\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": final_input_string}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": (\n",
    "                    \"You are an agricultural assistant that answers any user question about crops and diseases.\"\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": final_input_string}\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66c98e",
   "metadata": {
    "id": "8d66c98e",
    "outputId": "f034a3f1-7cc4-4a10-b740-695eb6208d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Structured JSON Mode ---\n",
      "{\n",
      "  \"crop\": \"Corn\",\n",
      "  \"disease\": \"Gray_leaf_spot\",\n",
      "  \"symptoms\": \"Gray leaf spot on corn typically presents as elongated lesions with grayish to tan centers and dark borders, generally appearing on the older leaves. The leaves may show yellowing around the spots, and extensive damage can lead to premature leaf death.\",\n",
      "  \"cure\": \"To manage gray leaf spot, practice crop rotation and select resistant corn hybrids. Fungicides may also be applied at the appropriate time, particularly when conditions are favorable for disease development. Ensure proper nitrogen management and good air circulation in the crop.\"\n",
      "}\n",
      "\n",
      "--- Free VQA Mode ---\n",
      "Question: Can this disease affect other crops like tea? Ans in Bangla\n",
      "Answer: গ্রে লিফ স্পট (Gray Leaf Spot) একটি রোগ যা সাধারণত ভুট্টা এবং কিছু অন্যান্য শষ্যের ওপর প্রভাব ফেলে। তবে, এটি চা গাছের (Tea plant) ওপর সরাসরি প্রভাব ফেলার কোনো প্রমাণ নেই। প্রতিটি শস্যের নিজস্ব রোগবাহক এবং পরিবেশগত চাহিদা থাকে, তাই এই রোগ চা গাছের জন্য সমস্যা হওয়ার সম্ভাবনা কম। তবে, চাষিদের উচিত সবসময় সতর্কতা অবলম্বন করা এবং রোগ-প্রতিরোধক ব্যবস্থা নেওয়া।\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Cell 9: Example Usage\n",
    "# ==============================\n",
    "test_img = r\"d:\\T2430458\\dataset\\Corn\\Gray_leaf_spot\\33333.png\"\n",
    "\n",
    "if os.path.exists(test_img):\n",
    "    # JSON Mode\n",
    "    gpt_input_json = build_gpt_input(test_img, crop_model, transform)\n",
    "    gpt_response_json = ask_gpt(gpt_input_json, json_mode=True)\n",
    "    parsed_json = json.loads(gpt_response_json)\n",
    "\n",
    "    crop_name, disease_name = predict_crop_and_disease(test_img, crop_model, transform)\n",
    "\n",
    "    result_json = {\n",
    "        \"crop\": crop_name,\n",
    "        \"disease\": disease_name,\n",
    "        \"symptoms\": parsed_json.get(\"symptoms\", \"N/A\"),\n",
    "        \"cure\": parsed_json.get(\"cure\", \"N/A\")\n",
    "    }\n",
    "    print(\"\\n--- Structured JSON Mode ---\")\n",
    "    print(json.dumps(result_json, indent=2))\n",
    "\n",
    "    # Free VQA Mode\n",
    "    user_question = \"Can this disease affect other crops like tea? Ans in Bangla\"\n",
    "    gpt_input_vqa = build_gpt_input(test_img, crop_model, transform, user_question=user_question)\n",
    "    gpt_response_vqa = ask_gpt(gpt_input_vqa, json_mode=False)\n",
    "    print(\"\\n--- Free VQA Mode ---\")\n",
    "    print(\"Question:\", user_question)\n",
    "    print(\"Answer:\", gpt_response_vqa)\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Test image not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
